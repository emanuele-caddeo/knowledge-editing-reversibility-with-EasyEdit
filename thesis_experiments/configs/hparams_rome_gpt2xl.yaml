alg_name: ROME
model_name: gpt2-xl
v_weight_decay: 0.0
kl_factor: 0.0
mom2_adjustment: true
mom2_dataset: wikipedia
mom2_n_samples: 10 #10000
mom2_dtype: float32
layers:
- 17
fact_token: subject_last
v_num_grad_steps: 20
v_lr: 0.1
v_loss_layer: 0
clamp_norm_factor: 4.0
rewrite_module_tmp: transformer.h.{}.mlp.c_proj
layer_module_tmp: transformer.h.{}
mlp_module_tmp: transformer.h.{}.mlp
attn_module_tmp: transformer.h.{}.attn
ln_f_module: transformer.ln_f
lm_head_module: lm_head
context_template_length_params:
- - 5
  - 5
- - 10
  - 10
stats_dir: thesis_experiments\stats
device: 0
